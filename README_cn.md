# âœ¨MuSc (ICLR 2024)âœ¨

**è®ºæ–‡â€œMuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Imagesâ€çš„å®˜æ–¹å¤ç°ä»£ç **

ä½œè€…:  [æç…¦è•¤](https://github.com/xrli-U)<sup>1*</sup> | [é»„å­é¸£](https://github.com/ZimingHuang1)<sup>1*</sup> | [è–›å³°](https://xuefeng-cvr.github.io/)<sup>3</sup> | [å‘¨ç‘œ](https://github.com/zhouyu-hust)<sup>1,2</sup>

å•ä½: <sup>1</sup>åä¸­ç§‘æŠ€å¤§å­¦ | <sup>2</sup>æ­¦æ±‰ç²¾æµ‹ç”µå­é›†å›¢è‚¡ä»½æœ‰é™å…¬å¸ | <sup>3</sup>ç‰¹ä¼¦æ‰˜å¤§å­¦

### ğŸ§ è®ºæ–‡ä¸‹è½½åœ°å€ï¼š [Arxiv](https://arxiv.org/pdf/2401.16753.pdf) | [OpenReview](https://openreview.net/forum?id=AHgc5SMdtd)

## <a href='#all_catelogue'>**è½¬åˆ°ç›®å½•**</a>

## ğŸ™ˆTODO list:
- â¬œï¸ ä½¿ç”¨ä¸€äº›ç­–ç•¥é™ä½æ¯å¼ å›¾çš„æ¨ç†é€Ÿåº¦ï¼Œä»955.3msé™ä½åˆ°**249.8ms**ã€‚
- â¬œï¸ å…¼å®¹æ›´å¤šçš„å·¥ä¸šæ•°æ®é›†ã€‚
- â¬œï¸ å…¼å®¹æ›´å¤šçš„è§†è§‰ç‰¹å¾æå–å™¨ï¼Œå¦‚ [Vision Mamba](https://github.com/hustvl/Vim)ã€‚

## ğŸ“£æ›´æ–°æ—¥å¿—:
***04/11/2024***
1. ä¸CVPR 2024ä¸­æœ€æ–°çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ–¹æ³•çš„ç»“æœæ¯”è¾ƒæ·»åŠ åˆ°äº†<a href='#compare_sota'>é›¶æ ·æœ¬/å°‘æ ·æœ¬æ–¹æ³•çš„å¯¹æ¯”ç»“æœ</a>ä¸­ã€‚
2. ä¿®å¤äº†`models/backbone/_backbones.py`ä¸­çš„ä¸€äº›bugã€‚

***2024å¹´3æœˆ22æ—¥***
1. æˆ‘ä»¬æä¾›äº†æ”¯æŒ[BTAD](https://ieeexplore.ieee.org/abstract/document/9576231)æ•°æ®é›†çš„è¿è¡Œä»£ç ï¼Œè¯¦è§`scripts/musc.sh`ã€‚
2. æˆ‘ä»¬ä¿®æ”¹äº†ä»£ç ä½¿å…¶æ”¯æŒæ›´å¤§çš„*batch_size*ã€‚
3. æˆ‘ä»¬ä¼˜åŒ–äº†éƒ¨åˆ†ä»£ç å®ç°æ›´å¿«çš„é€Ÿåº¦ã€‚
4. æˆ‘ä»¬æä¾›äº†åœ¨MVTec ADï¼ŒVisAå’ŒBTADæ•°æ®é›†ä¸Šï¼Œ<a href='#results_backbones'>ä½¿ç”¨ä¸åŒç‰¹å¾æå–å™¨çš„ç»“æœ</a>ã€‚
5. æˆ‘ä»¬æä¾›äº†ä¸åŒæ•°æ®é›†çš„<a href='#results_datasets'>è¯¦ç»†ç»“æœ</a>ã€‚
6. æˆ‘ä»¬æä¾›äº†ä¸åŒç‰¹å¾æå–å™¨çš„<a href='#inference_time'>æ¨ç†æ—¶é—´</a>ã€‚
7. æˆ‘ä»¬æä¾›äº†ä¸å½“å‰æœ€å…ˆè¿›çš„<a href='#compare_sota'>é›¶æ ·æœ¬/å°‘æ ·æœ¬æ–¹æ³•çš„å¯¹æ¯”ç»“æœ</a>ï¼Œè¯¥è¡¨æ ¼å°†ä¼šæŒç»­æ›´æ–°ã€‚
8. æˆ‘ä»¬æ±‡æ€»äº†ç”¨æˆ·åœ¨ä½¿ç”¨MuScè¿‡ç¨‹ä¸­åé¦ˆå›çš„<a href='#FAQ'>å¸¸è§é—®é¢˜</a>ï¼Œå¹¶ç»™å‡ºäº†è§£ç­”ã€‚

***2024å¹´2æœˆ1æ—¥***

åˆå§‹ç‰ˆæœ¬ï¼š

1. æˆ‘ä»¬æä¾›äº†[è¯¥è®ºæ–‡](https://arxiv.org/pdf/2401.16753.pdf)ä¸­çš„**MuSc**æ–¹æ³•å®Œæ•´å®ç°ä»£ç ã€‚
2. è¯¥ä»£ç çš„ç‰¹å¾æå–å™¨æ”¯æŒ[CLIP](https://github.com/mlfoundations/open_clip)ã€[DINO](https://github.com/facebookresearch/dino)å’Œ[DINO_v2](https://github.com/facebookresearch/dinov2)é¢„è®­ç»ƒçš„vision transformerã€‚

<span id='compare_sota'/>

## ğŸ–ï¸ä¸å…¶å®ƒé›¶æ ·æœ¬/å°‘æ ·æœ¬æ–¹æ³•çš„å¯¹æ¯”ç»“æœ <a href='#all_catelogue'>[è½¬åˆ°ç›®å½•]</a>
æˆ‘ä»¬åœ¨ä¸‹è¡¨ä¸­å±•ç¤ºäº†æˆ‘ä»¬çš„MuScæ–¹æ³•ä¸å½“å‰æœ€å…ˆè¿›çš„é›¶æ ·æœ¬/å°‘æ ·æœ¬æ–¹æ³•çš„å¯¹æ¯”ç»“æœï¼Œ**è¯¥è¡¨æ ¼å°†æŒç»­æ›´æ–°**ã€‚
"-"è¡¨ç¤ºä½œè€…åœ¨åŸè®ºæ–‡ä¸­æœªæä¾›è¯¥æŒ‡æ ‡ã€‚

### MVTec AD

|                                          |                    |         | Classification |            |        | Segmentation |             |         |          |
| :--------------------------------------: | :----------------: | :-----: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|                 Methods                  |       Venue        | Setting |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|                MuSc(ours)                |     ICLR 2024      | 0-shot  |      97.8      |    97.5    |  99.1  |     97.3     |    62.6     |  62.7   |   93.8   |
| [RegAD](https://link.springer.com/chapter/10.1007/978-3-031-20053-3_18) |     ECCV 2022      | 4-shot  |      89.1      |    92.4    |  94.9  |     96.2     |    51.7     |  48.3   |   88.0   |
| [GraphCore](https://openreview.net/forum?id=xzmqxHdZAwO) |     ICLR 2023      | 4-shot  |      92.9      |     -      |   -    |     97.4     |      -      |    -    |    -     |
| [WinCLIP](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) |     CVPR 2023      | 0-shot  |      91.8      |    92.9    |  96.5  |     85.1     |    31.7     |    -    |   64.6   |
| [WinCLIP](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) |     CVPR 2023      | 4-shot  |      95.2      |    94.7    |  97.3  |     96.2     |    51.7     |    -    |   88.0   |
| [APRIL-GAN](https://arxiv.org/pdf/2305.17382.pdf) | CVPR Workshop 2023 | 0-shot  |      86.1      |    90.4    |  93.5  |     87.6     |    43.3     |  40.8   |   44.0   |
| [APRIL-GAN](https://arxiv.org/pdf/2305.17382.pdf) | CVPR Workshop 2023 | 4-shot  |      92.8      |    92.8    |  96.3  |     95.9     |    56.9     |  54.5   |   91.8   |
| [FastRecon](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_FastRecon_Few-shot_Industrial_Anomaly_Detection_via_Fast_Feature_Reconstruction_ICCV_2023_paper.pdf) |     ICCV 2023      | 4-shot  |      94.2      |     -      |   -    |     97.0     |      -      |    -    |    -     |
| [ACR](https://proceedings.neurips.cc/paper_files/paper/2023/file/8078e8c3055303a884ffae2d3ea00338-Paper-Conference.pdf) |    NeurIPS 2023    | 0-shot  |      85.8      |    91.3    |  92.9  |     92.5     |    44.2     |  38.9   |   72.7   |
| [RegAD+Adversarial Loss](https://papers.bmvc2023.org/0202.pdf) |     BMVC 2023      | 8-shot  |      91.9      |     -      |   -    |     96.9     |      -      |    -    |    -     |
| [PACKD](https://papers.bmvc2023.org/0259.pdf) |     BMVC 2023      | 8-shot  |      95.3      |     -      |   -    |     97.3     |      -      |    -    |    -     |
| [PromptAD](https://openaccess.thecvf.com/content/WACV2024/papers/Li_PromptAD_Zero-Shot_Anomaly_Detection_Using_Text_Prompts_WACV_2024_paper.pdf) |     WACV 2024      | 0-shot  |      90.8      |     -      |   -    |     92.1     |    36.2     |    -    |   72.8   |
| [AnomalyCLIP](https://openreview.net/forum?id=buC4E91xZE) |     ICLR 2024      | 0-shot  |      91.5      |     -      |  96.2  |     91.1     |      -      |    -    |   81.4   |
| [InCTRL](https://arxiv.org/pdf/2403.06495.pdf) |     CVPR 2024      | 8-shot  |      95.3      |     -      |   -    |      -       |      -      |    -    |    -     |
| [MVFA-AD](https://arxiv.org/pdf/2403.12570.pdf) |     CVPR 2024      | 4-shot  |      96.2      |     -      |   -    |     96.3     |
| [PromptAD](https://arxiv.org/pdf/2404.05231.pdf) |     CVPR 2024      | 4-shot  |      96.6      |     -      |   -    |     96.5     |      -      |    -    |    -     |

### VisA

|                                          |                    |         | Classification |            |        | Segmentation |             |         |          |
| :--------------------------------------: | :----------------: | :-----: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|                 Methods                  |       Venue        | Setting |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|                MuSc(ours)                |     ICLR 2024      | 0-shot  |      92.8      |    89.5    |  93.5  |     98.8     |    48.8     |  45.1   |   92.7   |
| [WinCLIP](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) |     CVPR 2023      | 0-shot  |      78.1      |    79.0    |  81.2  |     79.6     |    14.8     |    -    |   56.8   |
| [WinCLIP](https://openaccess.thecvf.com/content/CVPR2023/papers/Jeong_WinCLIP_Zero-Few-Shot_Anomaly_Classification_and_Segmentation_CVPR_2023_paper.pdf) |     CVPR 2023      | 4-shot  |      87.3      |    84.2    |  88.8  |     97.2     |    47.0     |    -    |   87.6   |
| [APRIL-GAN](https://arxiv.org/pdf/2305.17382.pdf) | CVPR Workshop 2023 | 0-shot  |      78.0      |    78.7    |  81.4  |     94.2     |    32.3     |  25.7   |   86.8   |
| [APRIL-GAN](https://arxiv.org/pdf/2305.17382.pdf) | CVPR Workshop 2023 | 4-shot  |      92.6      |    88.4    |  94.5  |     96.2     |    40.0     |  32.2   |   90.2   |
| [PACKD](https://papers.bmvc2023.org/0259.pdf) |     BMVC 2023      | 8-shot  |      87.5      |     -      |   -    |     97.9     |      -      |    -    |    -     |
| [AnomalyCLIP](https://openreview.net/forum?id=buC4E91xZE) |     ICLR 2024      | 0-shot  |      82.1      |     -      |  85.4  |     95.5     |      -      |    -    |   87.0   |
| [InCTRL](https://arxiv.org/pdf/2403.06495.pdf) |     CVPR 2024      | 8-shot  |      88.7      |     -      |   -    |      -       |      -      |    -    |    -     |
| [PromptAD](https://arxiv.org/pdf/2404.05231.pdf) |     CVPR 2024      | 4-shot  |      89.1      |     -      |   -    |     97.4     |      -      |    -    |    -     |

<span id='all_catelogue'/>

## ğŸ“–ç›®å½•

* <a href='#abstract'>1. è®ºæ–‡ä»‹ç»</a>
* <a href='#setup'>2. ä»£ç è¿è¡Œç¯å¢ƒé…ç½®</a>
* <a href='#datasets'>3. æ•°æ®é›†ä¸‹è½½</a>
  * <a href='#datatets_mvtec_ad'>MVTec AD</a>
  * <a href='#datatets_visa'>VisA</a>
  * <a href='#datatets_btad'>BTAD</a>
* <a href='#run_musc'>4. è¿è¡Œä»£ç </a>
* <a href='#rscin'>5. å•ç‹¬è¿è¡ŒRsCINåˆ†ç±»ä¼˜åŒ–æ¨¡å—</a>
* <a href='#results_datasets'>6. åœ¨ä¸åŒæ•°æ®ä¸Šçš„ç»“æœ</a>
* <a href='#results_backbones'>7. ä½¿ç”¨ä¸åŒç‰¹å¾æå–å™¨çš„ç»“æœ</a>
* <a href='#inference_time'>8. æ¨ç†æ—¶é—´</a>
* <a href='#FAQ'>9. å¸¸è§é—®é¢˜</a>
* <a href='#citation'>10. å¼•ç”¨æ ¼å¼</a>
* <a href='#thanks'>11. è‡´è°¢</a>
* <a href='#license'>12. ä½¿ç”¨è®¸å¯</a>

<span id='abstract'/>

## ğŸ‘‡è®ºæ–‡ä»‹ç»: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

è¯¥è®ºæ–‡ç ”ç©¶äº†å·¥ä¸šè§†è§‰é¢†åŸŸä¸­çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²ä»»åŠ¡ã€‚
é›¶æ ·æœ¬ï¼Œå³ä¸ä½¿ç”¨ä»»ä½•ä¸æµ‹è¯•å›¾åƒåŒæºçš„æœ‰æ ‡æ³¨å›¾åƒï¼Œä»¥å¾€çš„æ–¹æ³•åŸºäºCLIPçš„å›¾æ–‡å¯¹é½èƒ½åŠ›å’ŒSAMçš„æç¤ºå·¥ç¨‹ï¼Œå¿½ç•¥äº†æ— æ ‡ç­¾æµ‹è¯•å›¾åƒæœ¬èº«è•´å«çš„ä¸°å¯Œæ­£å¸¸å…ˆéªŒä¿¡æ¯ã€‚
æœ¬è®ºæ–‡çš„å…³é”®å‘ç°åœ¨äºå·¥ä¸šäº§å“å›¾åƒä¸­ï¼Œå›¾åƒçš„æ­£å¸¸åŒºåŸŸå¯ä»¥åœ¨å…¶ä»–æ— æ ‡æ³¨çš„å›¾åƒä¸­æ‰¾åˆ°ç›¸å¯¹å¤§é‡çš„ç›¸ä¼¼çš„æ­£å¸¸åŒºåŸŸï¼Œè€Œå¼‚å¸¸åŒºåŸŸåªèƒ½æ‰¾åˆ°å°‘é‡ç›¸ä¼¼çš„åŒºåŸŸã€‚
æˆ‘ä»¬åˆ©ç”¨è¿™ç§ç‰¹æ€§è®¾è®¡äº†ä¸€ç§æ–°çš„é›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹/åˆ†å‰²æ–¹æ³•MuScï¼Œè¯¥æ–¹æ³•çš„æ ¸å¿ƒåœ¨äºå¯¹æ— æ ‡æ³¨çš„å›¾åƒè¿›è¡Œç›¸äº’æ‰“åˆ†ï¼Œæ­£å¸¸åŒºåŸŸä¼šè¢«èµ‹äºˆè¾ƒä½çš„åˆ†æ•°ï¼Œå¼‚å¸¸åŒºåŸŸä¼šè¢«èµ‹äºˆè¾ƒé«˜çš„åˆ†æ•°ã€‚
è¯¥æ–¹æ³•ä¸éœ€è¦ä»»ä½•è¾…åŠ©æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œä¹Ÿä¸éœ€è¦é¢å¤–çš„æ–‡æœ¬æ¨¡æ€è¿›è¡Œæç¤ºã€‚

å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨å¤šèšåˆåº¦é‚»åŸŸèšåˆæ¨¡å—(**LNAMD**)æ¥è·å–èƒ½å¤Ÿè¡¨å¾ä¸åŒå¤§å°ç¼ºé™·çš„åŒºåŸŸçº§ç‰¹å¾ã€‚
ç„¶åæˆ‘ä»¬æå‡ºäº†äº’æ‰“åˆ†æ¨¡å—(**MSM**)ï¼Œä½¿ç”¨æ— æ ‡æ³¨å›¾åƒè¿›è¡Œç›¸äº’æ‰“åˆ†ï¼Œåˆ†æ•°è¶Šé«˜è¡¨ç¤ºè¯¥å›¾åƒåŒºåŸŸå¼‚å¸¸æ¦‚ç‡è¶Šå¤§ã€‚
æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåˆ†ç±»ä¼˜åŒ–æ¨¡å—ï¼Œåä¸ºå›¾åƒçº§å—é™é‚»åŸŸçš„é‡æ‰“åˆ†(**RsCIN**)ï¼Œæ¥ä¼˜åŒ–åˆ†ç±»ç»“æœï¼Œå‡å°‘å™ªå£°å¸¦æ¥çš„è¯¯æ£€ã€‚

æˆ‘ä»¬é€šè¿‡åœ¨MVTec ADå’ŒVisAæ•°æ®é›†ä¸Šçš„ä¼˜å¼‚æ€§èƒ½è¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œä¸å½“å‰SOTAé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒMuScåœ¨MVTec ADæ•°æ®é›†ä¸Šå®ç°äº†**21.1**%çš„PROæå‡(ä»72.7ï¼…åˆ°93.8ï¼…)ï¼Œåœ¨VisAä¸Šå®ç°äº†**19.4**%çš„APåˆ†å‰²æå‡å’Œ**14.7**%çš„AUROCåˆ†å‰²æå‡ã€‚
æ­¤å¤–ï¼Œæˆ‘ä»¬çš„é›¶æ ·æœ¬æ–¹æ³•ç”šè‡³ä¼˜äºå½“å‰å¤§å¤šæ•°å°‘æ ·æœ¬æ–¹æ³•ï¼Œå¹¶ä¸”ä¸ä¸€äº›æ— ç›‘ç£æ–¹æ³•ç›¸åª²ç¾ã€‚

![pipline](./assets/pipeline.png) 

## ğŸ˜Šä¸å…¶å®ƒé›¶æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ¯”è¾ƒ

![Compare_0](./assets/compare_zero_shot.png) 

## ğŸ˜Šä¸å…¶å®ƒå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•æ¯”è¾ƒ

![Compare_4](./assets/compare_few_shot.png) 

<span id='setup'/>

## ğŸ¯ä»£ç ç¯å¢ƒé…ç½®: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

### ç¯å¢ƒ:

- Python 3.8
- CUDA 11.7
- PyTorch 2.0.1

ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å…‹éš†è¯¥é¡¹ç›®åˆ°æœ¬åœ°:

```
git clone https://github.com/xrli-U/MuSc.git
```

åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ:

```
conda create --name musc python=3.8
conda activate musc
```

å®‰è£…ä¾èµ–åº“:

```
pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2
pip install -r requirements.txt
```

<span id='datasets'/>

## ğŸ‘‡æ•°æ®é›†ä¸‹è½½: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

æŠŠæ‰€æœ‰çš„æ•°æ®é›†æ”¾åœ¨`./data`æ–‡ä»¶å¤¹ä¸‹ã€‚

<span id='datatets_mvtec_ad'/>

### [MVTec AD](https://www.mvtec.com/company/research/datasets/mvtec-ad/)

```
data
|---mvtec_anomaly_detection
|-----|-- bottle
|-----|-----|----- ground_truth
|-----|-----|----- test
|-----|-----|----- train
|-----|-- cable
|-----|--- ...
```

<span id='datatets_visa'/>

### [VisA](https://amazon-visual-anomaly.s3.us-west-2.amazonaws.com/VisA_20220922.tar)

```
data
|----visa
|-----|-- split_csv
|-----|-----|--- 1cls.csv
|-----|-----|--- ...
|-----|-- candle
|-----|-----|--- Data
|-----|-----|-----|----- Images
|-----|-----|-----|--------|------ Anomaly 
|-----|-----|-----|--------|------ Normal 
|-----|-----|-----|----- Masks
|-----|-----|-----|--------|------ Anomaly 
|-----|-----|--- image_anno.csv
|-----|-- capsules
|-----|--- ...
```

VisA datasetéœ€è¦ä½¿ç”¨å¦‚ä¸‹ç¨‹åºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚

```
python ./datasets/visa_preprocess.py
```

<span id='datatets_btad'/>

### [BTAD](https://github.com/pankajmishra000/VT-ADL)

```
data
|---btad
|-----|--- 01
|-----|-----|----- ground_truth
|-----|-----|----- test
|-----|-----|----- train
|-----|--- 02
|-----|--- ...
```

<span id='run_musc'/>

## ğŸ’è¿è¡Œä¸»ç¨‹åº: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

æˆ‘ä»¬æä¾›äº†ä¸¤ç§æ–¹å¼è¿è¡Œæˆ‘ä»¬çš„ä»£ç ã€‚

### pythonè¿è¡Œ

```
python examples/musc_main.py
```
éµå¾ª`./configs/musc.yaml`ä¸­çš„è®¾ç½®ã€‚

### shellè¿è¡Œ

```
sh scripts/musc.sh
```
è„šæœ¬`musc.sh`ä¸­çš„è®¾ç½®å…·æœ‰æ›´é«˜çš„ä¼˜å…ˆçº§ã€‚

å…³é”®å‚æ•°å¦‚ä¸‹ï¼š

- `--device`: GPU_idã€‚
- `--data_path`: æ•°æ®é›†è·¯å¾„ã€‚
- `--dataset_name`: æ•°æ®é›†åç§°ã€‚
- `--class_name`: è¿›è¡Œæµ‹è¯•çš„ç±»åˆ«ï¼Œå¦‚æœè¯¥å‚æ•°è®¾ç½®ä¸º`ALL`ï¼Œå°†å¯¹æ‰€æœ‰çš„ç±»åˆ«è¿›è¡Œæµ‹è¯•ã€‚
- `--backbone_name`: ç‰¹å¾æå–å™¨çš„åç§°ï¼Œæˆ‘ä»¬çš„ä»£ç å…¼å®¹CLIPï¼ŒDINOå’ŒDINO_v2ï¼Œè¯¦è§`configs/musc.yaml`ã€‚
- `--pretrained`: é€‰æ‹©é¢„è®­ç»ƒçš„CLIPæ¨¡å‹ï¼Œå¯é€‰`openai`ï¼Œ`laion400m_e31`å’Œ`laion400m_e32`ã€‚
- `--feature_layers`: backboneä¸­ç”¨äºæå–ç‰¹å¾çš„å±‚ã€‚
- `--img_resize`: è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å›¾åƒå¤§å°ã€‚
- `--divide_num`: å°†å®Œæ•´çš„æ— æ ‡ç­¾æµ‹è¯•é›†åˆ’åˆ†ä¸ºå­é›†çš„æ•°é‡ã€‚
- `--r_list`: LNAMDæ¨¡å—ä¸­çš„å¤šä¸ªèšåˆåº¦ã€‚
- `--output_dir`: ä¿å­˜è¯¥æ–¹æ³•é¢„æµ‹çš„å¼‚å¸¸æ¦‚ç‡å›¾å’Œæ£€æµ‹åˆ†å‰²æŒ‡æ ‡çš„è·¯å¾„ã€‚
- `--vis`: æ˜¯å¦ä¿å­˜è¯¥æ–¹æ³•é¢„æµ‹çš„å¼‚å¸¸æ¦‚ç‡å›¾ã€‚
- `--vis_type`: å¯åœ¨`single_norm`å’Œ`whole_norm`ä¸­è¿›è¡Œé€‰æ‹©ï¼Œ`single_norm`æ„æ€æ˜¯å°†æ¯å¼ å¼‚å¸¸æ¦‚ç‡å›¾è¿›è¡Œå½’ä¸€åŒ–åå†å¯è§†åŒ–ï¼Œ`whole_norm`æ„æ€æ˜¯å°†å…¨éƒ¨å¼‚å¸¸æ¦‚ç‡å›¾ç»Ÿä¸€è¿›è¡Œå½’ä¸€åŒ–åå†å¯è§†åŒ–ã€‚
- `--save_excel`: æ˜¯å¦ä¿å­˜è¯¥æ–¹æ³•å¼‚å¸¸æ£€æµ‹å’Œåˆ†å‰²çš„æŒ‡æ ‡ã€‚

<span id='rscin'/>

## ğŸ’åˆ†ç±»ä¼˜åŒ–æ¨¡å—RsCIN: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

å¯¹äºæˆ‘ä»¬çš„RsCINæ¨¡å—ï¼Œæˆ‘ä»¬åœ¨`./models/RsCIN_features`æ–‡ä»¶å¤¹ä¸­æä¾›äº†é¢å¤–çš„ä»£ç æ–¹ä¾¿å®ç°è¿ç§»ã€‚
æˆ‘ä»¬ä½¿ç”¨*ViT-large-14-336 of CLIP*æå–äº†MVTec ADå’ŒVisAæ•°æ®é›†çš„å›¾åƒçº§ç‰¹å¾ï¼Œå¹¶åˆ†åˆ«å­˜å‚¨åœ¨`mvtec_ad_cls.dat`å’Œ`visa_cls.dat`ä¸­ï¼Œ
åœ¨`./models/RsCIN_features/RsCIN.py`æ–‡ä»¶ä¸­æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚

### ä½¿ç”¨æ ·ä¾‹
åœ¨ä½¿ç”¨æˆ‘ä»¬çš„RsCINæ¨¡å—ä¹‹å‰ï¼Œè¯·å°†`RsCIN.py`ã€`mvtec_ad_cls.dat`å’Œ`visa_cls.dat`ç§»åŠ¨åˆ°è‡ªå·±çš„é¡¹ç›®ç›®å½•ä¸‹ã€‚

```
import numpy as np
from RsCIN import Mobile_RsCIN

classification_results = np.random.rand(83) # ä¼˜åŒ–å‰çš„åˆ†ç±»ç»“æœ
dataset_name = 'mvtec_ad' # æ•°æ®é›†åç§°
class_name = 'bottle' # æ•°æ®é›†ä¸­çš„äº§å“ç±»åˆ«
optimized_classification_results = Mobile_RsCIN(classification_results, dataset_name=dataset_name, class_name=class_name)
```

`optimized_classification_results`è¡¨ç¤ºç»è¿‡æˆ‘ä»¬çš„RsCINæ¨¡å—ä¼˜åŒ–ä¹‹åçš„åˆ†ç±»ç»“æœã€‚

### Apply to the custom dataset

å¦‚ä¸‹ï¼Œå¯¹äºå…¶å®ƒè‡ªå®šä¹‰çš„æ•°æ®é›†ï¼Œæ‚¨å¯ä»¥æå–æ¯å¼ å›¾åƒçš„å›¾åƒçº§ç‰¹å¾ï¼Œå¹¶å­˜å‚¨åœ¨å˜é‡`cls_tokens`ä¸­ï¼Œå¤šçª—å£æ©è†œæ“ä½œä¸­çš„å¤šä¸ªçª—å£å¤§å°å¯ä»¥é€šè¿‡æ”¹å˜å˜é‡`k_list`çš„å€¼è¿›è¡Œè°ƒæ•´ã€‚

```
import numpy as np
from RsCIN import Mobile_RsCIN

classification_results = np.random.rand(83) # the ä¼˜åŒ–å‰çš„åˆ†ç±»ç»“æœ
cls_tokens = np.random.rand(83, 768)  # shape[N, C] å›¾åƒçº§ç‰¹å¾, Nä¸ºå›¾åƒçš„æ•°é‡
k_list = [2, 3] # å¤šçª—å£æ©è†œæ“ä½œä¸­çš„å¤šä¸ªçª—å£å¤§å°
optimized_classification_results = Mobile_RsCIN(classification_results, k_list=k_list, cls_tokens=cls_tokens)
```

<span id='results_datasets'/>

## ğŸ–ï¸ä¸åŒæ•°æ®é›†çš„ç»“æœ: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

ä»¥ä¸‹æ‰€æœ‰çš„ç»“æœå‡æŒ‰ç…§è®ºæ–‡ä¸­çš„é»˜è®¤è®¾ç½®å¤ç°ã€‚

### MVTec AD

|            | Classification |            |        | Segmentation |             |         |          |
| :--------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|  Category  |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|   bottle   |     99.92      |   99.21    | 99.98  |    98.48     |    79.17    |  83.04  |  96.10   |
|   cable    |     98.99      |   97.30    | 99.42  |    95.76     |    60.97    |  57.70  |  89.62   |
|  capsule   |     96.45      |   94.88    | 99.30  |    98.96     |    49.80    |  48.45  |  95.49   |
|   carpet   |     99.88      |   99.44    | 99.96  |    99.45     |    73.33    |  76.05  |  97.58   |
|    grid    |     98.66      |   96.49    | 99.54  |    98.16     |    43.94    |  38.24  |  93.92   |
|  hazelnut  |     99.61      |   98.55    | 99.79  |    99.38     |    73.41    |  73.28  |  92.24   |
|  leather   |     100.0      |   100.0    | 100.0  |    99.72     |    62.84    |  64.47  |  98.74   |
| metal_nut  |     96.92      |   97.38    | 99.25  |    86.12     |    46.22    |  47.54  |  89.34   |
|    pill    |     96.24      |   95.89    | 99.31  |    97.47     |    65.54    |  67.25  |  98.01   |
|   screw    |     82.17      |   88.89    | 90.88  |    98.77     |    41.87    |  36.12  |  94.40   |
|    tile    |     100.0      |   100.0    | 100.0  |    97.90     |    74.71    |  78.90  |  94.64   |
| toothbrush |     100.0      |   100.0    | 100.0  |    99.53     |    70.19    |  67.79  |  95.48   |
| transistor |     99.42      |   95.00    | 99.19  |    91.38     |    59.24    |  58.40  |  77.21   |
|    wood    |     98.51      |   98.33    | 99.52  |    97.24     |    68.64    |  74.75  |  94.50   |
|   zipper   |     99.84      |   99.17    | 99.96  |    98.40     |    62.48    |  61.89  |  94.46   |
|    mean    |     97.77      |   97.37    | 99.07  |    97.11     |    62.16    |  62.26  |  93.45   |

### VisA

|            | Classification |            |        | Segmentation |             |         |          |
| :--------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|  Category  |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|   candle   |     96.55      |   91.26    | 96.45  |    99.36     |    39.56    |  28.36  |  97.62   |
|  capsules  |     88.62      |   86.43    | 93.77  |    98.71     |    50.85    |  43.90  |  88.20   |
|   cashew   |     98.54      |   95.57    | 99.30  |    99.33     |    74.88    |  77.63  |  94.30   |
| chewinggum |     98.42      |   96.45    | 99.30  |    99.54     |    61.33    |  61.21  |  88.39   |
|   fryum    |     98.64      |   97.44    | 99.43  |    99.43     |    58.13    |  50.43  |  94.38   |
| macaroni1  |     89.33      |   82.76    | 88.64  |    99.51     |    21.90    |  15.25  |  96.37   |
| macaroni2  |     68.03      |   69.96    | 67.37  |    97.14     |    11.06    |  3.91   |  88.84   |
|    pcb1    |     89.28      |   84.36    | 89.89  |    99.50     |    80.49    |  88.36  |  92.76   |
|    pcb2    |     93.20      |   88.66    | 94.46  |    97.39     |    34.38    |  21.86  |  86.06   |
|    pcb3    |     93.52      |   86.92    | 93.48  |    98.05     |    40.23    |  41.03  |  92.32   |
|    pcb4    |     98.43      |   92.89    | 98.47  |    98.70     |    46.38    |  44.72  |  92.66   |
| pipe_fryum |     98.34      |   96.04    | 99.16  |    99.40     |    48.90    |  67.90  |  97.32   |
|    mean    |     92.57      |   89.06    | 93.31  |    98.71     |    67.56    |  45.38  |  92.43   |

### BTAD

|          | Classification |            |        | Segmentation |             |         |          |
| :------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
| Category |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|    01    |     98.74      |   97.96    | 99.53  |    97.49     |    59.73    |  58.76  |  85.05   |
|    02    |     90.23      |   95.38    | 98.41  |    95.36     |    58.20    |  55.16  |  68.64   |
|    03    |     99.52      |   88.37    | 95.62  |    99.20     |    55.64    |  57.53  |  96.62   |
|   mean   |     96.16      |   93.90    | 97.85  |    97.35     |    57.86    |  57.15  |  83.43   |

<span id='results_backbones'/>

## ğŸ–ï¸ä½¿ç”¨ä¸åŒç‰¹å¾æå–å™¨çš„ç»“æœ: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

æˆ‘ä»¬è®ºæ–‡ä¸­ä½¿ç”¨çš„é»˜è®¤ç‰¹å¾æå–å™¨æ˜¯CLIPçš„ViT-large-14-336ã€‚
æˆ‘ä»¬è¿˜æä¾›äº†CLIPã€DINOå’ŒDINO_v2çš„vision transformerä½œä¸ºç‰¹å¾æå–å™¨çš„è¿è¡Œç¨‹åºï¼Œå…·ä½“ä¿¡æ¯è¯¦è§`configs/musc.yaml`ã€‚

### MVTec AD

|                   |              |            | Classification |            |        | Segmentation |             |         |          |
| :---------------: | :----------: | :--------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|     Backbones     | Pre-training | image size |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|     ViT-B-32      |     CLIP     |    256     |     87.99      |   92.31    | 94.38  |    93.08     |    42.06    |  37.21  |  72.62   |
|     ViT-B-32      |     CLIP     |    512     |     89.91      |   92.72    | 95.12  |    95.73     |    53.32    |  52.33  |  83.72   |
|     ViT-B-16      |     CLIP     |    256     |     92.78      |   93.98    | 96.59  |    96.21     |    52.48    |  50.23  |  87.00   |
|     ViT-B-16      |     CLIP     |    512     |     94.20      |   95.20    | 97.34  |    97.09     |    61.24    |  61.45  |  91.67   |
| ViT-B-16-plus-240 |     CLIP     |    240     |     94.77      |   95.43    | 97.60  |    96.26     |    52.23    |  50.27  |  87.70   |
| ViT-B-16-plus-240 |     CLIP     |    512     |     95.69      |   96.50    | 98.11  |    97.28     |    60.71    |  61.29  |  92.14   |
|     ViT-L-14      |     CLIP     |    336     |     96.06      |   96.65    | 98.25  |    97.24     |    59.41    |  58.10  |  91.69   |
|     ViT-L-14      |     CLIP     |    518     |     95.94      |   96.32    | 98.30  |    97.42     |    63.06    |  63.67  |  92.92   |
|   ViT-L-14-336    |     CLIP     |    336     |     96.40      |   96.44    | 98.30  |    97.03     |    57.51    |  55.44  |  92.18   |
|   ViT-L-14-336    |     CLIP     |    518     |     97.77      |   97.37    | 99.07  |    97.11     |    62.16    |  62.26  |  93.45   |
|  dino_vitbase16   |     DINO     |    256     |     89.39      |   93.77    | 95.37  |    95.83     |    54.02    |  52.84  |  84.24   |
|  dino_vitbase16   |     DINO     |    512     |     94.11      |   96.13    | 97.26  |    97.78     |    62.07    |  63.20  |  92.49   |
|   dinov2_vitb14   |   DINO_v2    |    336     |     95.67      |   96.80    | 97.95  |    97.74     |    60.23    |  59.45  |  93.84   |
|   dinov2_vitb14   |   DINO_v2    |    518     |     96.31      |   96.87    | 98.32  |    98.07     |    64.65    |  65.31  |  95.59   |
|   dinov2_vitl14   |   DINO_v2    |    336     |     96.84      |   97.45    | 98.68  |    98.17     |    61.77    |  61.21  |  94.62   |
|   dinov2_vitl14   |   DINO_v2    |    518     |     97.08      |   97.13    | 98.82  |    98.34     |    66.15    |  67.39  |  96.16   |


### VisA

|                   |              |            | Classification |            |        | Segmentation |             |         |          |
| :---------------: | :----------: | :--------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|     Backbones     | Pre-training | image size |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|     ViT-B-32      |     CLIP     |    256     |     72.95      |   76.90    | 77.68  |    89.30     |    25.93    |  20.68  |  50.95   |
|     ViT-B-32      |     CLIP     |    512     |     77.82      |   80.20    | 81.01  |    96.06     |    34.72    |  30.20  |  73.08   |
|     ViT-B-16      |     CLIP     |    256     |     81.44      |   80.86    | 83.84  |    95.97     |    36.72    |  31.81  |  73.48   |
|     ViT-B-16      |     CLIP     |    512     |     86.48      |   84.12    | 88.05  |    97.98     |    42.21    |  37.29  |  85.10   |
| ViT-B-16-plus-240 |     CLIP     |    240     |     82.62      |   81.61    | 85.05  |    96.11     |    37.84    |  33.43  |  72.37   |
| ViT-B-16-plus-240 |     CLIP     |    512     |     86.72      |   84.22    | 89.41  |    97.95     |    43.27    |  37.68  |  83.52   |
|     ViT-L-14      |     CLIP     |    336     |     88.38      |   85.23    | 89.77  |    98.32     |    44.67    |  40.42  |  87.80   |
|     ViT-L-14      |     CLIP     |    518     |     90.86      |   87.75    | 91.66  |    98.45     |    45.74    |  42.09  |  89.93   |
|   ViT-L-14-336    |     CLIP     |    336     |     88.61      |   85.31    | 90.00  |    98.53     |    45.10    |  40.92  |  89.35   |
|   ViT-L-14-336    |     CLIP     |    518     |     92.57      |   89.06    | 93.31  |    98.71     |    48.90    |  45.38  |  92.43   |
|  dino_vitbase16   |     DINO     |    256     |     78.21      |   80.12    | 81.11  |    95.74     |    36.81    |  32.84  |  70.21   |
|  dino_vitbase16   |     DINO     |    512     |     84.11      |   83.52    | 85.91  |    97.74     |    42.86    |  38.27  |  83.00   |
|   dinov2_vitb14   |   DINO_v2    |    336     |     87.65      |   86.24    | 88.51  |    97.80     |    41.68    |  37.06  |  85.01   |
|   dinov2_vitb14   |   DINO_v2    |    518     |     90.25      |   87.48    | 90.86  |    98.66     |    45.56    |  41.23  |  91.80   |
|   dinov2_vitl14   |   DINO_v2    |    336     |     90.18      |   88.47    | 90.56  |    98.38     |    43.84    |  38.74  |  88.38   |
|   dinov2_vitl14   |   DINO_v2    |    518     |     91.73      |   89.20    | 92.27  |    98.78     |    47.12    |  42.79  |  92.40   |


### BTAD

|                   |              |            | Classification |            |        | Segmentation |             |         |          |
| :---------------: | :----------: | :--------: | :------------: | :--------: | :----: | :----------: | :---------: | :-----: | :------: |
|     Backbones     | Pre-training | image size |   AUROC-cls    | F1-max-cls | AP-cls |  AUROC-segm  | F1-max-segm | AP-segm | PRO-segm |
|     ViT-B-32      |     CLIP     |    256     |     92.19      |   95.55    | 98.47  |    96.74     |    43.98    |  35.70  |  68.56   |
|     ViT-B-32      |     CLIP     |    512     |     93.31      |   94.61    | 98.40  |    97.41     |    52.94    |  48.80  |  69.59   |
|     ViT-B-16      |     CLIP     |    256     |     92.44      |   91.00    | 97.31  |    97.45     |    55.27    |  52.19  |  72.68   |
|     ViT-B-16      |     CLIP     |    512     |     94.11      |   92.99    | 97.98  |    97.91     |    59.18    |  59.05  |  77.86   |
| ViT-B-16-plus-240 |     CLIP     |    240     |     92.86      |   93.99    | 97.96  |    97.68     |    54.81    |  51.33  |  73.47   |
| ViT-B-16-plus-240 |     CLIP     |    512     |     94.13      |   93.84    | 98.34  |    98.14     |    58.66    |  57.53  |  77.23   |
|     ViT-L-14      |     CLIP     |    336     |     92.74      |   93.21    | 97.71  |    97.84     |    56.60    |  55.94  |  77.01   |
|     ViT-L-14      |     CLIP     |    518     |     94.82      |   95.29    | 98.58  |    97.77     |    55.55    |  55.46  |  80.62   |
|   ViT-L-14-336    |     CLIP     |    336     |     95.11      |   94.48    | 98.53  |    97.42     |    56.75    |  55.23  |  79.63   |
|   ViT-L-14-336    |     CLIP     |    518     |     96.16      |   93.90    | 97.85  |    97.35     |    57.86    |  57.15  |  83.43   |
|  dino_vitbase16   |     DINO     |    256     |     93.63      |   95.66    | 98.66  |    97.55     |    52.16    |  49.25  |  72.86   |
|  dino_vitbase16   |     DINO     |    512     |     92.38      |   92.66    | 97.81  |    97.44     |    53.32    |  53.02  |  74.91   |
|   dinov2_vitb14   |   DINO_v2    |    336     |     93.60      |   91.65    | 97.19  |    98.08     |    63.28    |  65.32  |  74.35   |
|   dinov2_vitb14   |   DINO_v2    |    518     |     94.99      |   95.11    | 98.55  |    98.30     |    65.75    |  68.89  |  80.41   |
|   dinov2_vitl14   |   DINO_v2    |    336     |     94.15      |   92.64    | 97.61  |    98.19     |    63.86    |  66.03  |  76.33   |
|   dinov2_vitl14   |   DINO_v2    |    518     |     95.62      |   95.40    | 98.76  |    98.40     |    65.88    |  69.90  |  82.47   |

<span id='inference_time'/>

## âŒ›æ¨ç†æ—¶é—´: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

åœ¨ä¸‹è¡¨ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä½¿ç”¨ä¸ç”¨backboneå’Œimage sizeæ—¶çš„æ¨ç†é€Ÿåº¦ã€‚
åœ¨è®¡ç®—æ¨ç†é€Ÿåº¦æ—¶ï¼Œæˆ‘ä»¬è®¾å®šä¸€æ¬¡æ€§å‚ä¸äº’æ‰“åˆ†çš„å›¾åƒæ•°é‡ä¸º**200**ï¼Œæ‰€ç”¨GPUä¸ºå•å¡NVIDIA RTX 3090ã€‚

|                   |              |            |                 |
| :---------------: | :----------: | :--------: | :-------------: |
|     Backbones     | Pre-training | image size | times(ms/image) |
|     ViT-B-32      |     CLIP     |    256     |      48.33      |
|     ViT-B-32      |     CLIP     |    512     |      95.74      |
|     ViT-B-16      |     CLIP     |    256     |      86.68      |
|     ViT-B-16      |     CLIP     |    512     |      450.5      |
| ViT-B-16-plus-240 |     CLIP     |    240     |      85.25      |
| ViT-B-16-plus-240 |     CLIP     |    512     |      506.4      |
|     ViT-L-14      |     CLIP     |    336     |      266.0      |
|     ViT-L-14      |     CLIP     |    518     |      933.3      |
|   ViT-L-14-336    |     CLIP     |    336     |      270.2      |
|   ViT-L-14-336    |     CLIP     |    518     |      955.3      |
|  dino_vitbase16   |     DINO     |    256     |      85.97      |
|  dino_vitbase16   |     DINO     |    512     |      458.5      |
|   dinov2_vitb14   |   DINO_v2    |    336     |      209.1      |
|   dinov2_vitb14   |   DINO_v2    |    518     |      755.0      |
|   dinov2_vitl14   |   DINO_v2    |    336     |      281.4      |
|   dinov2_vitl14   |   DINO_v2    |    518     |     1015.1      |

<span id='FAQ'/>

## ğŸ™‹ğŸ™‹â€â™‚ï¸å¸¸è§é—®é¢˜: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

Q: å¯è§†åŒ–å›¾ä¸­æ­£å¸¸çš„å›¾åƒä¸Šä¸ºä»€ä¹ˆä¼šå‡ºç°å¤§é¢ç§¯è¾ƒé«˜çš„å¼‚å¸¸åˆ†æ•°ï¼Ÿ

A: åœ¨å¯è§†åŒ–æ—¶ï¼Œä¸ºäº†çªå‡ºå¼‚å¸¸åŒºåŸŸï¼Œæˆ‘ä»¬é»˜è®¤é‡‡ç”¨äº†å•å›¾å½’ä¸€åŒ–ï¼Œå³ä¾¿å•å›¾å“åº”æ•´ä½“è¾ƒä½ï¼Œç»è¿‡å½’ä¸€åŒ–åä¹Ÿä¼šå‡ºç°å¤§é‡çš„é«˜äº®åŒºåŸŸã€‚å¯é€šè¿‡åœ¨shellè„šæœ¬ä¸­æ·»åŠ `vis_type`å‚æ•°ï¼Œå¹¶è®¾ç½®ä¸º`whole_norm`æ¥è¿›è¡Œå…¨éƒ¨å›¾åƒä¸€åŒå½’ä¸€åŒ–ï¼Œä¹Ÿå¯é€šè¿‡ä¿®æ”¹`./configs/musc.yaml`é…ç½®æ–‡ä»¶ä¸­çš„`testing->vis_type`å‚æ•°æ¥å®ç°ç›¸åŒçš„æ•ˆæœã€‚

Q: è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å›¾åƒåˆ†è¾¨ç‡å¦‚ä½•é€‰å–ï¼Ÿ

A: è¾“å…¥åˆ°æ¨¡å‹ä¸­çš„å›¾åƒåˆ†è¾¨ç‡`img_resize`ä¸€èˆ¬ä¸ºViT patch sizeçš„å€æ•°ï¼Œå¯ä»¥é˜²æ­¢è¾¹ç¼˜éƒ¨åˆ†äº§ç”Ÿè¯¯æ£€ï¼Œå¸¸ç”¨çš„å€¼ä¸º224ã€240ã€256ã€336ã€512ã€518ï¼Œæˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚<a href='#results_backbones'>*(è·³è½¬)*</a>ä¸­å±•ç¤ºäº†ä¸åŒç‰¹å¾æå–å™¨å¸¸ç”¨çš„ä¸¤ç§è¾“å…¥å›¾åƒåˆ†è¾¨ç‡çš„å¤§å°ï¼Œå¯ä¾›å‚è€ƒã€‚
å¯é€šè¿‡ä¿®æ”¹shellè„šæœ¬ä¸­çš„`img_resize`å‚æ•°æ›´æ”¹å›¾åƒåˆ†è¾¨ç‡ï¼Œä¹Ÿå¯é€šè¿‡ä¿®æ”¹`./configs/musc.yaml`é…ç½®æ–‡ä»¶ä¸­çš„`datasets->img_resize`å‚æ•°æ¥æ›´æ”¹ã€‚



<span id='citation'/>

## å¼•ç”¨: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>
```
@inproceedings{Li2024MuSc,
  title={MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images},
  author={Li, Xurui and Huang, Ziming and Xue, Feng and Zhou, Yu},
  booktitle={International Conference on Learning Representations},
  year={2024}
}
```

<span id='thanks'/>

## è‡´è°¢: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>

Our repo is built on [PatchCore](https://github.com/amazon-science/patchcore-inspection) and [APRIL-GAN](https://github.com/ByChelsea/VAND-APRIL-GAN), thanks their clear and elegant code !

<span id='license'/>

## ä½¿ç”¨è®¸å¯: <a href='#all_catelogue'>[è¿”å›ç›®å½•]</a>
MuSc is released under theÂ **MIT Licence**, and is fully open for academic research and also allow free commercial usage. To apply for a commercial license, please contact yuzhou@hust.edu.cn.
